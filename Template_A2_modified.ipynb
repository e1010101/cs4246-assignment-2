{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCNr2rW27nBk"
      },
      "source": [
        "# Team information\n",
        "\n",
        "| Team member 1     | Details     | Team member 2     | Details     |\n",
        "| :---------------- | :---------: | :---------------- | :---------: |\n",
        "| Name              | John Doe    | Name              | Jane Smith  |\n",
        "| NUSNet (Exxxxxxx) | E1234567    | NUSNet (Exxxxxxx) | E7654321    |\n",
        "| Matric (AxxxxxxxZ)| A0123456Z   | Matric (AxxxxxxxZ)| A0654321Z   |\n",
        "\n",
        "*Note: Fill in your actual details.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NQp3ObqD9d4u"
      },
      "outputs": [],
      "source": [
        "# Connect to Google drive to save your model, etc.,\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Set a path in your Google Drive to save models and plots\n",
        "    # Example: '/content/drive/MyDrive/CS4246_A2/'\n",
        "    SAVE_PATH = '/content/drive/MyDrive/CS4246_A2/' \n",
        "    import os\n",
        "    if not os.path.exists(SAVE_PATH):\n",
        "        os.makedirs(SAVE_PATH)\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Not running in Google Colab. Make sure SAVE_PATH is set correctly.\")\n",
        "    # Set a local path if not using Colab\n",
        "    SAVE_PATH = './cs4246_a2_results/' \n",
        "    import os\n",
        "    if not os.path.exists(SAVE_PATH):\n",
        "        os.makedirs(SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCmfJeb_BMHM"
      },
      "source": [
        "# Installation and setup\n",
        "\n",
        "The gym environment requires an older version numpy (and corresponding packages). <br>\n",
        "The following cell contains the `requirements.txt` to setup the python environment used in the rest of this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWitd3VTBgwU",
        "outputId": "6ca9a583-0516-4e58-b298-a5d346a55e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "# Using cloudpickle 2.2.1 compatible with numpy 1.24.2\n",
        "cloudpickle==2.2.1\n",
        "contourpy==1.0.7 # Compatible with matplotlib 3.7.1\n",
        "cycler==0.11.0\n",
        "filelock==3.9.0 # Common dependency\n",
        "fonttools==4.39.0\n",
        "fsspec==2023.1.0 # Common dependency\n",
        "gym==0.26.2\n",
        "gym-notices==0.0.8\n",
        "importlib_metadata==6.0.0\n",
        "# importlib_resources may cause issues, try without first\n",
        "Jinja2==3.1.2\n",
        "kiwisolver==1.4.4\n",
        "MarkupSafe==2.1.2\n",
        "matplotlib==3.7.1 # Adjusted version\n",
        "mpmath==1.2.1\n",
        "networkx==3.0 # Adjusted version\n",
        "numpy==1.24.2\n",
        "# Nvidia packages might differ based on CUDA version, these are examples\n",
        "# nvidia-cublas-cu11 # Or cu12 depending on your env\n",
        "# nvidia-cuda-cupti-cu11\n",
        "# nvidia-cuda-nvrtc-cu11\n",
        "# nvidia-cuda-runtime-cu11\n",
        "# nvidia-cudnn-cu11\n",
        "# nvidia-cufft-cu11\n",
        "# nvidia-curand-cu11\n",
        "# nvidia-cusolver-cu11\n",
        "# nvidia-cusparse-cu11\n",
        "# nvidia-nccl-cu11\n",
        "# nvidia-nvtx-cu11\n",
        "packaging==23.0\n",
        "pillow==9.4.0\n",
        "ply==3.11\n",
        "pygame==2.3.0 # Adjusted version\n",
        "pyparsing==3.0.9\n",
        "python-dateutil==2.8.2\n",
        "six==1.16.0\n",
        "sympy==1.11.1\n",
        "torch>=1.13.1 # Ensure torch is compatible\n",
        "torchaudio>=0.13.1\n",
        "torchvision>=0.14.1\n",
        "tqdm==4.64.1\n",
        "# triton might not be needed unless using specific GPU features\n",
        "zipp==3.15.0\n",
        "\n",
        "# Added based on pyRDDLGym setup.py\n",
        "scipy>=1.8.0\n",
        "pillow>=9.1.0\n",
        "matplotlib>=3.5.2\n",
        "pygame>=2.1.2\n",
        "gym>=0.26.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_9eK2YJBoGb"
      },
      "source": [
        "Now install the requirements.\n",
        "\n",
        "You may be asked to restart the session to load the installed versions of the packages. If so, restart the session and continue using the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fXtGcN8u94_N"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt --force-reinstall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13c63a6"
      },
      "source": [
        "We will use a discretized version of\n",
        "the [elevator domain](https://ataitler.github.io/IPPC2023/elevator.html) from the International Planning Competition, 2023.\n",
        "\n",
        "Install the pyRDDL gym environment using the given repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8U02_AG3900U"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/tasbolat1/pyRDDLGym.git --force-reinstall\n",
        "\n",
        "## Install other packages if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-gknJ0Ud97HT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, namedtuple\n",
        "import math\n",
        "import os\n",
        "\n",
        "# Try importing MovieGenerator, handle potential error if cv2 not found\n",
        "try:\n",
        "    from pyRDDLGym.Visualizer.MovieGenerator import MovieGenerator # loads visualizer utilites\n",
        "except UserWarning as e:\n",
        "    print(f\"Caught UserWarning: {e}\")\n",
        "    print(\"Movie generation might be disabled if cv2 is missing.\")\n",
        "    MovieGenerator = None # Disable movie generation if problematic\n",
        "except ImportError as e:\n",
        "     print(f\"Caught ImportError: {e}\")\n",
        "     print(\"Movie generation might be disabled if dependencies are missing.\")\n",
        "     MovieGenerator = None\n",
        "\n",
        "from IPython.display import Image, display, clear_output # for displaying gifs in colab\n",
        "from pyRDDLGym.Elevator import Elevator # imports Discrete Elevator\n",
        "\n",
        "## Add more imports here as required\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGHWCiCnfCO4"
      },
      "source": [
        "# Environment Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o1E0mIDq-LXu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Examples d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Examples\\manifest.csv\n",
            "Available example environment(s):\n",
            "CartPole_continuous -> A simple continuous state-action MDP for the classical cart-pole system by Rich Sutton, with actions that describe the continuous force applied to the cart.\n",
            "CartPole_discrete -> A simple continuous state MDP for the classical cart-pole system by Rich Sutton, with discrete actions that apply a constant force on either the left or right side of the cart.\n",
            "Elevators -> The Elevator domain models evening rush hours when people from different floors in a building want to go down to the bottom floor using elevators.\n",
            "HVAC -> Multi-zone and multi-heater HVAC control problem\n",
            "MarsRover -> Multi Rover Navigation, where a group of agent needs to harvest mineral.\n",
            "MountainCar -> A simple continuous MDP for the classical mountain car control problem.\n",
            "NewLanguage -> Example with new language features.\n",
            "NewtonZero -> Example with Newton root-finding method.\n",
            "PowerGen_continuous -> A continuous simple power generation problem loosely modeled on the problem of unit commitment.\n",
            "PowerGen_discrete -> A simple power generation problem loosely modeled on the problem of unit commitment.\n",
            "PropDBN -> Simple propositional DBN.\n",
            "RaceCar -> A simple continuous MDP for the racecar problem.\n",
            "RecSim -> A problem of recommendation systems, with consumers and providers.\n",
            "Reservoir_continuous -> Continuous action version of management of the water level in interconnected reservoirs.\n",
            "Reservoir_discrete -> Discrete version of management of the water level in interconnected reservoirs.\n",
            "SupplyChain -> A supply chain with factory and multiple warehouses.\n",
            "SupplyChainNet -> A supply chain network with factory and multiple warehouses.\n",
            "Traffic -> BLX/QTM traffic model.\n",
            "UAV_continuous -> Continuous action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
            "UAV_discrete -> Discrete action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
            "UAV_mixed -> Mixed action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
            "Wildfire -> A boolean version of the wildfire fighting domain.\n",
            "The building has 5 floors and 1 elevators. Each floor has maximum 3 people waiting. Each elevator can carry maximum of 10 people.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Core\\Env\\RDDLConstraints.py:85: UserWarning: Constraint does not have a structure of <action or state fluent> <op> <rhs>, where:\n",
            "<op> is one of {<=, <, >=, >}\n",
            "<rhs> is a deterministic function of non-fluents or constants only.\n",
            ">> ( sum_{?f: floor} [ elevator-at-floor(?e, ?f) ] ) == 1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete environment actions:\n",
            "{0: ('e0_movcurdir_0',), 1: ('e0_movcurdir_1',), 2: ('e0_close_0',), 3: ('e0_close_1',), 4: ('e0_open_0',), 5: ('e0_open_1',)}\n",
            "Continuous environment actions:\n",
            "Dict('move-current-dir___e0': Discrete(2), 'open-door___e0': Discrete(2), 'close-door___e0': Discrete(2))\n",
            "Observation space size for the discrete Elevator Environment: 225280\n",
            "Number of discrete actions: 6\n"
          ]
        }
      ],
      "source": [
        "## IMPORTANT: Do not change the instance of the environment.\n",
        "env = Elevator(instance = 5)\n",
        "\n",
        "print('Discrete environment actions:')\n",
        "print(env.disc_actions)\n",
        "print('Continuous environment actions:')\n",
        "print(env.base_env.action_space)\n",
        "print(f\"Observation space size for the discrete Elevator Environment: {len(env.disc_states)}\")\n",
        "print(f\"Number of discrete actions: {env.action_space.n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13dca8b"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZrE28ZRGBmk"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "\n",
        "## IMPORTANT: <BEGIN> DO NOT CHANGE THIS CODE!\n",
        "## GENERAL HYPERPARAMS\n",
        "num_episodes = 3000\n",
        "## IMPORTANT: <END> DO NOT CHANGE THIS CODE!\n",
        "\n",
        "## DQN HYPERPARAMS\n",
        "learning_rate = 1e-4       # Learning rate for Adam optimizer\n",
        "batch_size = 128           # Number of transitions sampled from the replay buffer\n",
        "gamma = 0.99               # Discount factor for future rewards\n",
        "epsilon_start = 1.0        # Starting value of epsilon for epsilon-greedy exploration\n",
        "epsilon_end = 0.05         # Minimum value of epsilon\n",
        "epsilon_decay_steps = 50000 # Number of steps over which epsilon decays linearly\n",
        "tau = 0.005                # Soft update parameter for target network\n",
        "buffer_size = 50000        # Maximum size of the replay buffer\n",
        "target_update_freq = 100   # How often to update the target network (in steps)\n",
        "min_buffer_size_for_training = 1000 # Minimum buffer size before training starts\n",
        "hidden_size = 128          # Number of neurons in hidden layers\n",
        "clip_value = 1.0           # Gradient clipping value (set to None to disable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53155607"
      },
      "source": [
        "# Model Definition\n",
        "\n",
        "Define your model here. You can rename the class `YourModel` appropriately and use it later in the code.\n",
        "Note: In case of actor-critic or other models, all components must subclass `nn.Module`\n",
        "\n",
        "- Your model should take in 11 inputs, which will be derived from the convert_state_to_list function.\n",
        "- Your model should return 6 values corresponding to the Q-values for each action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2bLSWBgLGEVC"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size=128):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # Simple Multi-Layer Perceptron (MLP)\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size) # Output Q-values for each action\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x will be a tensor with shape [batch_size, input_size]\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        q_values = self.fc3(x) # Linear output layer for Q-values\n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "replay_buffer_markdown"
      },
      "source": [
        "# Replay Buffer\n",
        "\n",
        "Stores transitions and allows sampling batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "replay_buffer_code"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        self.buffer.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d501d6e1"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c96b0591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment features (13): ['num-person-waiting___f0', 'num-person-waiting___f1', 'num-person-waiting___f2', 'num-person-waiting___f3', 'num-person-waiting___f4', 'num-person-in-elevator___e0', 'elevator-dir-up___e0', 'elevator-closed___e0', 'elevator-at-floor___e0__f0', 'elevator-at-floor___e0__f1', 'elevator-at-floor___e0__f2', 'elevator-at-floor___e0__f3', 'elevator-at-floor___e0__f4']\n",
            "\n",
            "Initial state description: {'num-person-in-elevator___e0': 0, 'elevator-dir-up___e0': True, 'elevator-closed___e0': True, 'elevator-at-floor___e0__f0': True, 'elevator-at-floor___e0__f1': False, 'elevator-at-floor___e0__f2': False, 'elevator-at-floor___e0__f3': False, 'elevator-at-floor___e0__f4': False, 'num-person-waiting___f0': 0, 'num-person-waiting___f1': 0, 'num-person-waiting___f2': 0, 'num-person-waiting___f3': 0, 'num-person-waiting___f4': 0}\n",
            "Converted initial state list (13): [0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "## IMPORTANT: DO NOT CHANGE THIS CODE!\n",
        "env_features = list(env.observation_space.keys())\n",
        "print(f\"Environment features ({len(env_features)}): {env_features}\")\n",
        "\n",
        "def convert_state_to_list(state_desc, env_features):\n",
        "    out = []\n",
        "    for i in env_features:\n",
        "        # Ensure boolean values are converted to floats (0.0 or 1.0)\n",
        "        value = state_desc[i]\n",
        "        if isinstance(value, bool):\n",
        "            out.append(float(value))\n",
        "        else:\n",
        "            out.append(value) # Assumes numerical values otherwise\n",
        "    return out\n",
        "\n",
        "# Example usage to verify\n",
        "initial_state = env.reset()\n",
        "initial_state_desc = env.disc2state(initial_state)\n",
        "initial_state_list = convert_state_to_list(initial_state_desc, env_features)\n",
        "print(f\"\\nInitial state description: {initial_state_desc}\")\n",
        "print(f\"Converted initial state list ({len(initial_state_list)}): {initial_state_list}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4a67d06"
      },
      "source": [
        "# Neural Net Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9uRwCjl7GHDJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network Input Size: 13\n",
            "Network Output Size: 6\n"
          ]
        }
      ],
      "source": [
        "# Determine input and output sizes from the environment\n",
        "input_size = len(env_features) # Should be 11 based on template\n",
        "output_size = env.action_space.n # Number of discrete actions (should be 6)\n",
        "\n",
        "print(f\"Network Input Size: {input_size}\")\n",
        "print(f\"Network Output Size: {output_size}\")\n",
        "\n",
        "# Initialize the Q-Network and Target Network\n",
        "policy_net = QNetwork(input_size, output_size, hidden_size)\n",
        "target_net = QNetwork(input_size, output_size, hidden_size)\n",
        "target_net.load_state_dict(policy_net.state_dict()) # Initialize target net with policy net weights\n",
        "target_net.eval() # Target network is only for inference\n",
        "\n",
        "# Initialize the optimizer - Adam is a good choice\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=learning_rate, amsgrad=True)\n",
        "\n",
        "# Initialize the replay buffer\n",
        "memory = ReplayBuffer(buffer_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xpEQ5uTqGJIQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (fc1): Linear(in_features=13, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert networks to CUDA if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "policy_net.to(device)\n",
        "target_net.to(device)\n",
        "\n",
        "# Define other constructs (replay buffers, etc) as necessary\n",
        "# Replay buffer 'memory' already initialized above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GooOycK-MPib"
      },
      "source": [
        "## Gradient Clipping (Optional, using torch's version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MZM5yTnHMN83"
      },
      "outputs": [],
      "source": [
        "# Define a function for gradient clipping (using PyTorch's built-in)\n",
        "def clip_grads_pytorch(model, max_norm):\n",
        "    if max_norm is not None:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "# We will call clip_grads_pytorch(policy_net, clip_value) after loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c50a3522"
      },
      "source": [
        "# Live Plotting Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kJTkOusq4bbH"
      },
      "outputs": [],
      "source": [
        "# Create a figure for plotting\n",
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "plt.ion() # Turn on interactive mode\n",
        "\n",
        "# Lists to store rewards and episode numbers\n",
        "rewards_list = []\n",
        "episodes_list = [] # Store episode numbers for x-axis\n",
        "losses_list = [] # Optional: store losses\n",
        "\n",
        "def exponential_smoothing(data, alpha=0.1):\n",
        "    \"\"\"Compute exponential smoothing.\"\"\"\n",
        "    if not data: return []\n",
        "    smoothed = [data[0]]  # Initialize with the first data point\n",
        "    for i in range(1, len(data)):\n",
        "        st = alpha * data[i] + (1 - alpha) * smoothed[-1]\n",
        "        smoothed.append(st)\n",
        "    return smoothed\n",
        "\n",
        "def live_plot(data_dict, figure, axes, ylabel=\"Total Rewards\", title=\"Training Progress\"):\n",
        "    \"\"\"Plot the live graph.\"\"\"\n",
        "    try:\n",
        "      clear_output(wait=True)\n",
        "      axes.clear()\n",
        "      colors = ['yellow', 'purple', 'blue', 'green'] # Add more colors if needed\n",
        "      color_idx = 0\n",
        "\n",
        "      for label, data_info in data_dict.items():\n",
        "          episodes = data_info['episodes']\n",
        "          values = data_info['values']\n",
        "          plot_smoothed = data_info.get('smooth', False)\n",
        "          y_label = data_info.get('ylabel', ylabel) # Allow specific y-label per plot\n",
        "\n",
        "          if not episodes or not values: continue # Skip if no data\n",
        "\n",
        "          # Plot raw data\n",
        "          axes.plot(episodes, values, label=label, color=colors[color_idx % len(colors)], linestyle='--' if plot_smoothed else '-', alpha=0.6 if plot_smoothed else 1.0)\n",
        "          color_idx += 1\n",
        "\n",
        "          # Compute and plot moving average if requested\n",
        "          if plot_smoothed:\n",
        "              smoothed_values = exponential_smoothing(values)\n",
        "              if smoothed_values:\n",
        "                  axes.plot(episodes, smoothed_values, label=f\"Smoothed {label}\", linestyle=\"-\", color=colors[color_idx % len(colors)], linewidth=2)\n",
        "                  color_idx += 1\n",
        "\n",
        "      axes.set_xlabel(\"Episode\")\n",
        "      axes.set_ylabel(y_label)\n",
        "      axes.set_title(title)\n",
        "      axes.legend(loc='upper left')\n",
        "      axes.grid(True)\n",
        "      display(figure)\n",
        "      plt.pause(0.01) # Small pause to allow plot to update\n",
        "    except Exception as e:\n",
        "        print(f\"Plotting error: {e}\") # Catch potential display errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "232d30e3"
      },
      "source": [
        "# RL Algorithm - DQN Specific Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fItfNTEx8Luf"
      },
      "outputs": [],
      "source": [
        "# Define the loss calculation function (part of the optimization step)\n",
        "def optimize_model():\n",
        "    if len(memory) < batch_size or len(memory) < min_buffer_size_for_training:\n",
        "        return None # Not enough samples yet\n",
        "\n",
        "    transitions = memory.sample(batch_size)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda d: not d, batch.done)), device=device, dtype=torch.bool)\n",
        "    # Ensure next_states that are None (for terminal states) are handled\n",
        "    non_final_next_states_list = [s for s in batch.next_state if s is not None]\n",
        "\n",
        "    # Check if there are any non-final states before creating tensor\n",
        "    if non_final_next_states_list:\n",
        "      non_final_next_states = torch.stack(non_final_next_states_list, dim=0)\n",
        "    else:\n",
        "      # If all states in the batch are terminal, create an empty tensor of the correct shape\n",
        "      non_final_next_states = torch.empty((0, input_size), device=device, dtype=torch.float32)\n",
        "\n",
        "    state_list = [s for s in batch.state] # Explicitly create list\n",
        "    state_batch = torch.stack(state_list, dim=0) # Shape (batch_size, 13)\n",
        "\n",
        "    action_batch = torch.cat(batch.action) # Shape (batch_size, 1)\n",
        "    reward_batch = torch.cat(batch.reward) # Shape (batch_size,)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(batch_size, device=device)\n",
        "    if non_final_next_states_list: # Only compute if there are non-final states\n",
        "        with torch.no_grad():\n",
        "            q_next = target_net(non_final_next_states)\n",
        "            next_state_values[non_final_mask] = q_next.max(1)[0]\n",
        "\n",
        "    # Compute the expected Q values (Bellman target)\n",
        "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
        "\n",
        "    # Compute Huber loss (less sensitive to outliers than MSE)\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient Clipping\n",
        "    clip_grads_pytorch(policy_net, clip_value)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mV629tHLL6GQ"
      },
      "outputs": [],
      "source": [
        "steps_done = 0\n",
        "\n",
        "def choose_action(state_tensor):\n",
        "    global steps_done\n",
        "    # Linearly decay epsilon\n",
        "    if epsilon_decay_steps > 0:\n",
        "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * \\\n",
        "            math.exp(-1. * steps_done / epsilon_decay_steps) # Exponential decay is often better\n",
        "        # Linear decay: max(epsilon_end, epsilon_start - (epsilon_start - epsilon_end) * steps_done / epsilon_decay_steps)\n",
        "    else:\n",
        "        epsilon = epsilon_end # If no decay steps, use end value\n",
        "\n",
        "    steps_done += 1\n",
        "\n",
        "    # Epsilon-greedy action selection\n",
        "    if random.random() > epsilon:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            q_values = policy_net(state_tensor.unsqueeze(0)) # Add batch dimension\n",
        "            action = q_values.max(1)[1].view(1, 1)\n",
        "            return action.item() # Return the integer action\n",
        "    else:\n",
        "        # Choose a random action\n",
        "        return random.randrange(output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKakuRs4ELDu"
      },
      "source": [
        "## Training loop with live plotting\n",
        "\n",
        "Use the graph generated here in your pdf submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAVE_PATH = 'save'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aVy8Zc8vGV7D"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x700 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final plot saved to save\\dqn_training_rewards.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ezrat\\AppData\\Local\\Temp\\ipykernel_55680\\134099025.py:114: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "plt.ion() # Turn on interactive mode\n",
        "\n",
        "# Reset lists for new training run\n",
        "rewards_list = []\n",
        "episodes_list = []\n",
        "losses_list = []\n",
        "current_loss = 0.0 # Track loss for display\n",
        "steps_done = 0 # Reset step counter for epsilon decay\n",
        "\n",
        "# Create a tqdm progress bar\n",
        "progress_bar = tqdm.tqdm(range(num_episodes), desc='Training Progress', postfix={'Total Reward': 0, 'Avg Loss': 0, 'Epsilon': epsilon_start})\n",
        "\n",
        "# RL algorithm training loop\n",
        "for episode in progress_bar:\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "    state_desc = env.disc2state(state)\n",
        "    state_list = convert_state_to_list(state_desc, env_features)\n",
        "    state_tensor = torch.tensor(state_list, dtype=torch.float32, device=device)\n",
        "\n",
        "    episode_losses = [] # Track losses within an episode\n",
        "\n",
        "    # The environment runs for a fixed horizon (env.horizon)\n",
        "    for t in range(env.horizon):\n",
        "\n",
        "        action_int = choose_action(state_tensor)\n",
        "        # Need action as tensor for buffer\n",
        "        action_tensor = torch.tensor([[action_int]], device=device, dtype=torch.long)\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        next_state, reward, done, info = env.step(action_int)\n",
        "        total_reward += reward\n",
        "        reward_tensor = torch.tensor([reward], device=device, dtype=torch.float32)\n",
        "\n",
        "        # Convert the next state to the suitable format for the network\n",
        "        if not done:\n",
        "            next_state_desc = env.disc2state(next_state)\n",
        "            next_state_list = convert_state_to_list(next_state_desc, env_features)\n",
        "            next_state_tensor = torch.tensor(next_state_list, dtype=torch.float32, device=device)\n",
        "        else:\n",
        "            next_state_tensor = None # Indicate terminal state\n",
        "\n",
        "        # Store the transition in memory\n",
        "        # Ensure state_tensor is used, not the list\n",
        "        memory.push(state_tensor, action_tensor, next_state_tensor, reward_tensor, done)\n",
        "\n",
        "        # Move to the next state\n",
        "        state_tensor = next_state_tensor\n",
        "        state = next_state # Keep track of discrete state if needed\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        loss_value = optimize_model()\n",
        "        if loss_value is not None:\n",
        "           episode_losses.append(loss_value)\n",
        "           current_loss = loss_value # Update loss for display\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        # If episode finished (though env runs for fixed horizon)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # After episode calculations\n",
        "    rewards_list.append(total_reward)\n",
        "    episodes_list.append(episode)\n",
        "    if episode_losses:\n",
        "        losses_list.append(np.mean(episode_losses))\n",
        "    else:\n",
        "        losses_list.append(None) # Append None if no training happened\n",
        "\n",
        "    # Update plot (e.g., every 10 episodes)\n",
        "    if episode % 10 == 0 or episode == num_episodes - 1:\n",
        "      plot_data = {\n",
        "          'Total Reward': {'episodes': episodes_list, 'values': rewards_list, 'smooth': True, 'ylabel': 'Total Reward'}\n",
        "          # Optionally plot loss\n",
        "          # 'Average Loss': {'episodes': episodes_list, 'values': [l for l in losses_list if l is not None], 'smooth': True, 'ylabel': 'Average Loss'}\n",
        "      }\n",
        "      live_plot(plot_data, fig, ax, title=f'Training Progress (Episode {episode+1}/{num_episodes})')\n",
        "\n",
        "    # Calculate current epsilon for display\n",
        "    current_epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * steps_done / epsilon_decay_steps) if epsilon_decay_steps > 0 else epsilon_end\n",
        "\n",
        "    # Update progress bar postfix\n",
        "    progress_bar.set_postfix({'Total Reward': f'{total_reward:.2f}',\n",
        "                              'Avg Loss': f'{current_loss:.4f}' if current_loss is not None else 'N/A',\n",
        "                              'Epsilon': f'{current_epsilon:.3f}',\n",
        "                              'Steps': steps_done})\n",
        "\n",
        "    # Saving the model periodically\n",
        "    if episode % 500 == 0 or episode == num_episodes - 1:\n",
        "      model_save_path = os.path.join(SAVE_PATH, f'dqn_elevator_model_ep{episode}.pt')\n",
        "      torch.save(policy_net.state_dict(), model_save_path)\n",
        "      print(f\"\\nModel saved to {model_save_path}\")\n",
        "\n",
        "# Final plot update after loop ends\n",
        "plot_data = {\n",
        "    'Total Reward': {'episodes': episodes_list, 'values': rewards_list, 'smooth': True, 'ylabel': 'Total Reward'}\n",
        "}\n",
        "live_plot(plot_data, fig, ax, title=f'Training Completed ({num_episodes} Episodes)')\n",
        "plt.ioff() # Turn off interactive mode\n",
        "\n",
        "# Save the final plot\n",
        "plot_save_path = os.path.join(SAVE_PATH, 'dqn_training_rewards.png')\n",
        "fig.savefig(plot_save_path)\n",
        "print(f\"\\nFinal plot saved to {plot_save_path}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp0VFEQpF57M"
      },
      "source": [
        "## Compute the mean rewards\n",
        "\n",
        "Report the mean rewards obtained over the last 100 episodes in your pdf submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yCL1WgMHF86n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Mean Reward over last 100 episodes: -1029.67\n"
          ]
        }
      ],
      "source": [
        "# Calculate mean reward over the last 100 episodes\n",
        "if len(rewards_list) >= 100:\n",
        "    mean_reward_last_100 = np.mean(rewards_list[-100:])\n",
        "else:\n",
        "    mean_reward_last_100 = np.mean(rewards_list) # Mean over all if less than 100 episodes\n",
        "\n",
        "print(f\"\\nMean Reward over last 100 episodes: {mean_reward_last_100:.2f}\")\n",
        "\n",
        "# close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation_markdown"
      },
      "source": [
        "# Evaluation (Optional but Recommended)\n",
        "\n",
        "Evaluate the trained policy without exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation_code"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy_net, env, num_eval_episodes=10, render=False, render_path=None):\n",
        "    policy_net.eval()  # Set the network to evaluation mode\n",
        "    total_rewards = []\n",
        "    env_features = list(env.observation_space.keys())\n",
        "\n",
        "    # Initialize visualizer if rendering\n",
        "    if render and MovieGenerator is not None and render_path is not None:\n",
        "        # Need to re-initialize env with rendering enabled for evaluation\n",
        "        eval_env = Elevator(instance=5, is_render=True, render_path=render_path)\n",
        "        print(f\"Rendering evaluation to {render_path}\")\n",
        "    else:\n",
        "        eval_env = env  # Use the original env if not rendering\n",
        "        render = False  # Ensure render is False if MovieGenerator failed\n",
        "\n",
        "    print(f\"\\nStarting evaluation for {num_eval_episodes} episodes...\")\n",
        "    for episode in range(num_eval_episodes):\n",
        "        state = eval_env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(eval_env.horizon):\n",
        "            state_desc = eval_env.disc2state(state)\n",
        "            state_list = convert_state_to_list(state_desc, env_features)\n",
        "            state_tensor = torch.tensor(state_list, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                q_values = policy_net(state_tensor)\n",
        "                action = q_values.max(1)[1].item()  # Choose action with highest Q-value\n",
        "\n",
        "            next_state, reward, done, _ = eval_env.step(action)\n",
        "\n",
        "            if render:\n",
        "                eval_env.render()\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "        print(f\"Evaluation Episode {episode + 1}/{num_eval_episodes}, Reward: {episode_reward:.2f}\")\n",
        "\n",
        "    if render:\n",
        "        try:\n",
        "            eval_env.save_render()\n",
        "            gif_path = os.path.join(render_path, 'elevator.gif')\n",
        "            print(f\"Evaluation GIF saved to {gif_path}\")\n",
        "            \n",
        "            # Check if the file exists before trying to display it\n",
        "            if os.path.exists(gif_path):\n",
        "                try:\n",
        "                    display(Image(filename=gif_path))\n",
        "                except NameError:\n",
        "                    print(\"Cannot display image directly. Check the saved file.\")\n",
        "            else:\n",
        "                print(f\"Warning: GIF file was not created at {gif_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during rendering: {e}\")\n",
        "            print(\"Continuing with evaluation results...\")\n",
        "        \n",
        "        eval_env.close()\n",
        "\n",
        "    mean_eval_reward = np.mean(total_rewards)\n",
        "    std_eval_reward = np.std(total_rewards)\n",
        "    print(f\"\\nEvaluation Results ({num_eval_episodes} episodes):\")\n",
        "    print(f\"Mean Reward: {mean_eval_reward:.2f} +/- {std_eval_reward:.2f}\")\n",
        "    policy_net.train()  # Set the network back to training mode\n",
        "    return mean_eval_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Examples d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Examples\\manifest.csv\n",
            "Available example environment(s):\n",
            "CartPole_continuous -> A simple continuous state-action MDP for the classical cart-pole system by Rich Sutton, with actions that describe the continuous force applied to the cart.\n",
            "CartPole_discrete -> A simple continuous state MDP for the classical cart-pole system by Rich Sutton, with discrete actions that apply a constant force on either the left or right side of the cart.\n",
            "Elevators -> The Elevator domain models evening rush hours when people from different floors in a building want to go down to the bottom floor using elevators.\n",
            "HVAC -> Multi-zone and multi-heater HVAC control problem\n",
            "MarsRover -> Multi Rover Navigation, where a group of agent needs to harvest mineral.\n",
            "MountainCar -> A simple continuous MDP for the classical mountain car control problem.\n",
            "NewLanguage -> Example with new language features.\n",
            "NewtonZero -> Example with Newton root-finding method.\n",
            "PowerGen_continuous -> A continuous simple power generation problem loosely modeled on the problem of unit commitment.\n",
            "PowerGen_discrete -> A simple power generation problem loosely modeled on the problem of unit commitment.\n",
            "PropDBN -> Simple propositional DBN.\n",
            "RaceCar -> A simple continuous MDP for the racecar problem.\n",
            "RecSim -> A problem of recommendation systems, with consumers and providers.\n",
            "Reservoir_continuous -> Continuous action version of management of the water level in interconnected reservoirs.\n",
            "Reservoir_discrete -> Discrete version of management of the water level in interconnected reservoirs.\n",
            "SupplyChain -> A supply chain with factory and multiple warehouses.\n",
            "SupplyChainNet -> A supply chain network with factory and multiple warehouses.\n",
            "Traffic -> BLX/QTM traffic model.\n",
            "UAV_continuous -> Continuous action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
            "UAV_discrete -> Discrete action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
            "UAV_mixed -> Mixed action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
            "Wildfire -> A boolean version of the wildfire fighting domain.\n",
            "The building has 5 floors and 1 elevators. Each floor has maximum 3 people waiting. Each elevator can carry maximum of 10 people.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Core\\Env\\RDDLConstraints.py:85: UserWarning: Constraint does not have a structure of <action or state fluent> <op> <rhs>, where:\n",
            "<op> is one of {<=, <, >=, >}\n",
            "<rhs> is a deterministic function of non-fluents or constants only.\n",
            ">> ( sum_{?f: floor} [ elevator-at-floor(?e, ?f) ] ) == 1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering evaluation to save\n",
            "True\n",
            "\n",
            "Starting evaluation for 5 episodes...\n",
            "Evaluation Episode 1/5, Reward: -522.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Visualizer\\MovieGenerator.py:83: UserWarning: removed 201 temporary files at save\\Elevators_*.png\n",
            "  self.reset()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Episode 2/5, Reward: -522.75\n",
            "Evaluation Episode 3/5, Reward: -1000.50\n",
            "Evaluation Episode 4/5, Reward: -1654.50\n",
            "Evaluation Episode 5/5, Reward: -1091.25\n",
            "Evaluation GIF saved to save\\elevator_eval.gif\n",
            "Warning: GIF file was not created at save\\elevator_eval.gif.\n",
            "\n",
            "Evaluation Results (5 episodes):\n",
            "Mean Reward: -958.35 +/- 420.37\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-958.35"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model_path = os.path.join(SAVE_PATH, 'dqn_elevator_model_ep2999.pt')\n",
        "policy_net.load_state_dict(torch.load(best_model_path))\n",
        "evaluate_policy(policy_net, env, num_eval_episodes=5, render=True, render_path=SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
