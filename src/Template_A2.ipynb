{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCNr2rW27nBk"
      },
      "source": [
        "# Team information\n",
        "\n",
        "| Team member 1     | Details  | Team member 2     | Details  |\n",
        "| :---------------- | :------: | :---------------- | :------: |\n",
        "| Name              |          | Name              |          |\n",
        "| NUSNet (Exxxxxxx) |          | NUSNet (Exxxxxxx) |          |\n",
        "| Matric (AxxxxxxxZ)|          | Matric (AxxxxxxxZ)|          |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQp3ObqD9d4u"
      },
      "outputs": [],
      "source": [
        "# Connect to Google drive to save your model, etc.,\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCmfJeb_BMHM"
      },
      "source": [
        "# Installation and setup\n",
        "\n",
        "The gym environment requires an older version numpy (and corresponding packages). <br>\n",
        "The following cell contains the `requirements.txt` to setup the python environment used in the rest of this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWitd3VTBgwU",
        "outputId": "6ca9a583-0516-4e58-b298-a5d346a55e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "cloudpickle==3.1.1\n",
        "contourpy==1.3.0\n",
        "cycler==0.12.1\n",
        "filelock==3.18.0\n",
        "fonttools==4.56.0\n",
        "fsspec==2025.3.0\n",
        "gym==0.26.2\n",
        "gym-notices==0.0.8\n",
        "importlib_metadata==8.6.1\n",
        "importlib_resources==6.5.2\n",
        "Jinja2==3.1.6\n",
        "kiwisolver==1.4.7\n",
        "MarkupSafe==3.0.2\n",
        "matplotlib==3.9.4\n",
        "mpmath==1.3.0\n",
        "networkx==3.2.1\n",
        "numpy==1.24.2\n",
        "nvidia-cublas-cu12==12.4.5.8\n",
        "nvidia-cuda-cupti-cu12==12.4.127\n",
        "nvidia-cuda-nvrtc-cu12==12.4.127\n",
        "nvidia-cuda-runtime-cu12==12.4.127\n",
        "nvidia-cudnn-cu12==9.1.0.70\n",
        "nvidia-cufft-cu12==11.2.1.3\n",
        "nvidia-curand-cu12==10.3.5.147\n",
        "nvidia-cusolver-cu12==11.6.1.9\n",
        "nvidia-cusparse-cu12==12.3.1.170\n",
        "nvidia-cusparselt-cu12==0.6.2\n",
        "nvidia-nccl-cu12==2.21.5\n",
        "nvidia-nvjitlink-cu12==12.4.127\n",
        "nvidia-nvtx-cu12==12.4.127\n",
        "packaging==24.2\n",
        "pillow==11.1.0\n",
        "ply==3.11\n",
        "pygame==2.6.1\n",
        "pyparsing==3.2.1\n",
        "python-dateutil==2.9.0.post0\n",
        "six==1.17.0\n",
        "sympy==1.13.1\n",
        "torch==2.6.0\n",
        "tqdm==4.67.1\n",
        "triton==3.2.0\n",
        "zipp==3.21.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_9eK2YJBoGb"
      },
      "source": [
        "Now install the requirements.\n",
        "\n",
        "You may be asked to restart the session to load the installed versions of the packages. If so, restart the session and continue using the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fXtGcN8u94_N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloudpickle==3.1.1 in d:\\github\\cs4246_assignment_2\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (3.1.1)\n",
            "Collecting contourpy==1.3.0 (from -r requirements.txt (line 3))\n",
            "  Using cached contourpy-1.3.0-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in d:\\github\\cs4246_assignment_2\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.12.1)\n",
            "Collecting filelock==3.18.0 (from -r requirements.txt (line 5))\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: fonttools==4.56.0 in d:\\github\\cs4246_assignment_2\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (4.56.0)\n",
            "Collecting fsspec==2025.3.0 (from -r requirements.txt (line 7))\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: gym==0.26.2 in d:\\github\\cs4246_assignment_2\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.26.2)\n",
            "Requirement already satisfied: gym-notices==0.0.8 in d:\\github\\cs4246_assignment_2\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (0.0.8)\n",
            "Collecting importlib_metadata==8.6.1 (from -r requirements.txt (line 10))\n",
            "  Using cached importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting importlib_resources==6.5.2 (from -r requirements.txt (line 11))\n",
            "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting Jinja2==3.1.6 (from -r requirements.txt (line 12))\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting kiwisolver==1.4.7 (from -r requirements.txt (line 13))\n",
            "  Using cached kiwisolver-1.4.7-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
            "Collecting MarkupSafe==3.0.2 (from -r requirements.txt (line 14))\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
            "Collecting matplotlib==3.9.4 (from -r requirements.txt (line 15))\n",
            "  Using cached matplotlib-3.9.4-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
            "Collecting mpmath==1.3.0 (from -r requirements.txt (line 16))\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting networkx==3.2.1 (from -r requirements.txt (line 17))\n",
            "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting numpy==1.24.2 (from -r requirements.txt (line 18))\n",
            "  Using cached numpy-1.24.2-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from -r requirements.txt (line 19))\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from -r requirements.txt (line 20))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from -r requirements.txt (line 21))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from -r requirements.txt (line 22))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from -r requirements.txt (line 23))\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from -r requirements.txt (line 24))\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from -r requirements.txt (line 25))\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from -r requirements.txt (line 26))\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from -r requirements.txt (line 27))\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from -r requirements.txt (line 28))\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-win_amd64.whl.metadata (6.8 kB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement nvidia-nccl-cu12==2.21.5 (from versions: 0.0.1.dev5)\n",
            "ERROR: No matching distribution found for nvidia-nccl-cu12==2.21.5\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13c63a6"
      },
      "source": [
        "We will use a discretized version of\n",
        "the [elevator domain](https://ataitler.github.io/IPPC2023/elevator.html) from the International Planning Competition, 2023.\n",
        "\n",
        "Install the pyRDDL gym environment using the given repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8U02_AG3900U"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/tasbolat1/pyRDDLGym.git --force-reinstall\n",
        "\n",
        "## Install other packages if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-gknJ0Ud97HT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ezrat\\AppData\\Local\\Temp\\ipykernel_5064\\1468883518.py:14: UserWarning: cv2 is not installed: save_as_mp4 option will be disabled.\n",
            "  from pyRDDLGym.Visualizer.MovieGenerator import MovieGenerator # loads visualizer utilites\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "from pyRDDLGym.Visualizer.MovieGenerator import MovieGenerator # loads visualizer utilites\n",
        "from IPython.display import Image, display, clear_output # for displaying gifs in colab\n",
        "from pyRDDLGym.Elevator import Elevator # imports Discrete Elevator\n",
        "\n",
        "## Add more imports here as required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGHWCiCnfCO4"
      },
      "source": [
        "# Environment Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o1E0mIDq-LXu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Examples d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Examples\\manifest.csv\n",
            "Available example environment(s):\n",
            "CartPole_continuous -> A simple continuous state-action MDP for the classical cart-pole system by Rich Sutton, with actions that describe the continuous force applied to the cart.\n",
            "CartPole_discrete -> A simple continuous state MDP for the classical cart-pole system by Rich Sutton, with discrete actions that apply a constant force on either the left or right side of the cart.\n",
            "Elevators -> The Elevator domain models evening rush hours when people from different floors in a building want to go down to the bottom floor using elevators.\n",
            "HVAC -> Multi-zone and multi-heater HVAC control problem\n",
            "MarsRover -> Multi Rover Navigation, where a group of agent needs to harvest mineral.\n",
            "MountainCar -> A simple continuous MDP for the classical mountain car control problem.\n",
            "NewLanguage -> Example with new language features.\n",
            "NewtonZero -> Example with Newton root-finding method.\n",
            "PowerGen_continuous -> A continuous simple power generation problem loosely modeled on the problem of unit commitment.\n",
            "PowerGen_discrete -> A simple power generation problem loosely modeled on the problem of unit commitment.\n",
            "PropDBN -> Simple propositional DBN.\n",
            "RaceCar -> A simple continuous MDP for the racecar problem.\n",
            "RecSim -> A problem of recommendation systems, with consumers and providers.\n",
            "Reservoir_continuous -> Continuous action version of management of the water level in interconnected reservoirs.\n",
            "Reservoir_discrete -> Discrete version of management of the water level in interconnected reservoirs.\n",
            "SupplyChain -> A supply chain with factory and multiple warehouses.\n",
            "SupplyChainNet -> A supply chain network with factory and multiple warehouses.\n",
            "Traffic -> BLX/QTM traffic model.\n",
            "UAV_continuous -> Continuous action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
            "UAV_discrete -> Discrete action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
            "UAV_mixed -> Mixed action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
            "Wildfire -> A boolean version of the wildfire fighting domain.\n",
            "The building has 5 floors and 1 elevators. Each floor has maximum 3 people waiting. Each elevator can carry maximum of 10 people.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\GitHub\\cs4246_assignment_2\\.venv\\Lib\\site-packages\\pyRDDLGym\\Core\\Env\\RDDLConstraints.py:85: UserWarning: Constraint does not have a structure of <action or state fluent> <op> <rhs>, where:\n",
            "<op> is one of {<=, <, >=, >}\n",
            "<rhs> is a deterministic function of non-fluents or constants only.\n",
            ">> ( sum_{?f: floor} [ elevator-at-floor(?e, ?f) ] ) == 1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete environment actions:\n",
            "{0: ('e0_movcurdir_0',), 1: ('e0_movcurdir_1',), 2: ('e0_close_0',), 3: ('e0_close_1',), 4: ('e0_open_0',), 5: ('e0_open_1',)}\n",
            "Continuous environment actions:\n",
            "Dict('move-current-dir___e0': Discrete(2), 'open-door___e0': Discrete(2), 'close-door___e0': Discrete(2))\n",
            "Observation space size for the discrete Elevator Environment: 225280\n"
          ]
        }
      ],
      "source": [
        "## IMPORTANT: Do not change the instance of the environment.\n",
        "env = Elevator(instance = 5)\n",
        "\n",
        "print('Discrete environment actions:')\n",
        "print(env.disc_actions)\n",
        "print('Continuous environment actions:')\n",
        "print(env.base_env.action_space)\n",
        "print(f\"Observation space size for the discrete Elevator Environment: {len(env.disc_states)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13dca8b"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uZrE28ZRGBmk"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "\n",
        "## IMPORTANT: <BEGIN> DO NOT CHANGE THIS CODE!\n",
        "## GENERAL HYPERPARAMS\n",
        "num_episodes = 3000\n",
        "## IMPORTANT: <END> DO NOT CHANGE THIS CODE!\n",
        "\n",
        "learning_rate = 3e-4\n",
        "batch_size = 64\n",
        "clip_value = 1.0  # Gradient clipping value\n",
        "\n",
        "## ALGO SPECIFIC HYPERPARAMS\n",
        "# Update the hyperparams as necessary for your implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53155607"
      },
      "source": [
        "# Model Definition\n",
        "\n",
        "Define your model here. You can rename the class `YourModel` appropriately and use it later in the code.\n",
        "Note: In case of actor-critic or other models, all components must subclass `nn.Module`\n",
        "\n",
        "- Your model should take in 11 inputs, which will be derived from the convert_state_to_list function.\n",
        "- Your model should return 6 values corresponding to action logits or probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bLSWBgLGEVC"
      },
      "outputs": [],
      "source": [
        "class YourModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YourModel, self).__init__()\n",
        "        # Your model layers and initializations here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x will be a tensor with shape [batch_size, 11]\n",
        "        # Your forward pass logic here\n",
        "        # Ensure the output has shape [batch_size, 6]\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d501d6e1"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c96b0591"
      },
      "outputs": [],
      "source": [
        "## IMPORTANT: DO NOT CHANGE THIS CODE!\n",
        "env_features = list(env.observation_space.keys())\n",
        "\n",
        "def convert_state_to_list(state, env_features):\n",
        "    out = []\n",
        "    for i in env_features:\n",
        "        out.append(state[i])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4a67d06"
      },
      "source": [
        "# Neural Net Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uRwCjl7GHDJ"
      },
      "outputs": [],
      "source": [
        "# Initialize the network and optimizer\n",
        "input_size = len(env_features)\n",
        "output_size = 6\n",
        "\n",
        "# INITIALIZE OTHER NETWORK PARAMS HERE\n",
        "hidden_size = ...\n",
        "\n",
        "# INITIALIZE YOUR NETWORK HERE\n",
        "your_network = YourModel()\n",
        "\n",
        "# INIT OPTIMIZER - Adam is a good start, but you can try changing this as well\n",
        "optimizer = optim.Adam(\n",
        "    your_network.parameters(), lr=learning_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpEQ5uTqGJIQ"
      },
      "outputs": [],
      "source": [
        "# Convert networks to CUDA if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "your_network.to(device)\n",
        "\n",
        "# Define other constructs (replay buffers, etc) as necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GooOycK-MPib"
      },
      "source": [
        "## Gradient Clipping (Optional, you can use torch's version as well)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZM5yTnHMN83"
      },
      "outputs": [],
      "source": [
        "# Define a function for gradient clipping\n",
        "def clip_grads(model, clip_value):\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            param.grad.data = torch.clamp(param.grad.data, -clip_value, clip_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c50a3522"
      },
      "source": [
        "# Live Plotting Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJTkOusq4bbH"
      },
      "outputs": [],
      "source": [
        "# Create a figure for plotting\n",
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plt.ion()\n",
        "\n",
        "# Lists to store rewards and episode numbers\n",
        "rewards_list = []\n",
        "episodes = []\n",
        "\n",
        "def exponential_smoothing(data, alpha=0.1):\n",
        "    \"\"\"Compute exponential smoothing.\"\"\"\n",
        "    smoothed = [data[0]]  # Initialize with the first data point\n",
        "    for i in range(1, len(data)):\n",
        "        st = alpha * data[i] + (1 - alpha) * smoothed[-1]\n",
        "        smoothed.append(st)\n",
        "    return smoothed\n",
        "\n",
        "def live_plot(data_dict, figure, ylabel=\"Total Rewards\"):\n",
        "    \"\"\"Plot the live graph.\"\"\"\n",
        "    clear_output(wait=True)\n",
        "    ax.clear()\n",
        "    for label, data in data_dict.items():\n",
        "        if label == \"Total Reward\":\n",
        "            ax.plot(data, label=label, color=\"yellow\", linestyle='--')\n",
        "\n",
        "            # Compute and plot moving average for total reward\n",
        "            ma = exponential_smoothing(data)\n",
        "            ma_idx_start = len(data) - len(ma)\n",
        "            ax.plot(range(ma_idx_start, len(data)), ma, label=\"Smoothed Value\", linestyle=\"-\", color=\"purple\", linewidth=2)\n",
        "        else:\n",
        "            ax.plot(data, label=label)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.legend(loc='upper left')\n",
        "    display(figure)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "232d30e3"
      },
      "source": [
        "# RL Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fItfNTEx8Luf"
      },
      "outputs": [],
      "source": [
        "# Define the loss calculation function\n",
        "def calculate_loss(\n",
        "    ## INCLUDE PARAMS YOU NEED HERE\n",
        "    ):\n",
        "    ## TODO - CALCULATE LOSS VALUE & RETURN IT\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV629tHLL6GQ"
      },
      "outputs": [],
      "source": [
        "def choose_action(\n",
        "    ## INCLUDE PARAMS YOU NEED HERE\n",
        "    ):\n",
        "    ## TODO - RETURN AN INTEGER FROM 0 - 5 (both inclusive) based on your model training/testing strategy\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKakuRs4ELDu"
      },
      "source": [
        "## Training loop with live plotting\n",
        "\n",
        "Use the graph generated here in your pdf submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVy8Zc8vGV7D"
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plt.ion()\n",
        "\n",
        "# Create a tqdm progress bar\n",
        "progress_bar = tqdm.tqdm(range(num_episodes), postfix={'Total Reward': 0, 'Loss': 0})\n",
        "\n",
        "# RL algorithm training loop\n",
        "for episode in progress_bar:\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "        # Convert the original state to the suitable format for the network\n",
        "        state_desc = env.disc2state(state)\n",
        "        state_list = convert_state_to_list(state_desc, env_features)\n",
        "        state_tensor = torch.tensor(state_list, dtype=torch.float32, device=device)\n",
        "\n",
        "        action = choose_action(\n",
        "            ## TODO: FILL IN PARAMS FOR CALLING choose_action\n",
        "        )\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        next_state, reward, done, _ = env.step((action))\n",
        "\n",
        "        # Convert the next state to the suitable format for the network\n",
        "        next_state_desc = env.disc2state(next_state)\n",
        "        next_state_list = convert_state_to_list(next_state_desc, env_features)\n",
        "        next_state_tensor = torch.tensor(next_state_list, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "        # Hint: You may want to collect experiences from the environment to update the agent in batches!\n",
        "\n",
        "        loss = calculate_loss(\n",
        "            ## TODO: FILL IN PARAMS FOR CALLING calculate_loss\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "    episodes.append(episode)\n",
        "\n",
        "    live_plot({'Total Reward': rewards_list}, fig)\n",
        "\n",
        "    # Saving the model\n",
        "    if episode%500 == 0:\n",
        "      torch.save(your_network, f'model.pt')\n",
        "\n",
        "    progress_bar.set_postfix({'Total Reward': total_reward, 'Loss': loss.item()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp0VFEQpF57M"
      },
      "source": [
        "## Compute the mean rewards\n",
        "\n",
        "Report the mean rewards obtained in your pdf submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCL1WgMHF86n"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nMean Rewards: ...\")\n",
        "\n",
        "# close the environment\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
